{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Question 2 - Property Type Distribution:**\n",
    "\n",
    "\n",
    "Using groupBy, create a count of listings by property type in Boston. Rename the resulting column to \"Property Type Count.\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Assuming you have a \"property_type\" and \"city\" column\n",
    "listing_df_new = listing_df.withColumnRenamed(\"property_type\", \"Property Type\")\n",
    "listing_df_new = listing_df_new.withColumnRenamed(\"city\", \"City\")\n",
    "\n",
    "# Filter the DataFrame to select only listings in Boston\n",
    "boston_listings = listing_df_new.filter(col(\"City\") == \"Boston\")\n",
    "\n",
    "# Group by \"Property Type\" and count the number of listings\n",
    "result_df = boston_listings.groupBy(\"Property Type\") \\\n",
    "    .agg(count(\"*\").alias(\"Property Type Count\"))\n",
    "\n",
    "# Show the resulting DataFrame with the renamed column\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Price Trend Over Time:**\n",
    "\n",
    "Calculate the average price change for each listing compared to the previous month.\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, avg \n",
    "\n",
    "\n",
    "# Define a window specification to order data by listing_id and date\n",
    "window_spec = Window.partitionBy(\"listing_id\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the lagged price (price of the previous month) for each listing\n",
    "calender_df_lag = calender_df.withColumn(\"lagged_price\", lag(\"price\").over(window_spec))\n",
    "\n",
    "# Calculate the price change by subtracting the lagged price from the current price\n",
    "calender_df_change = calender_df_lag.withColumn(\"price_change\", calender_df[\"price\"] - calender_df_lag[\"lagged_price\"])\n",
    "\n",
    "# Group by listing_id and calculate the average price change\n",
    "average_price_change = calender_df_change.groupBy(\"listing_id\").agg(avg(\"price_change\").alias(\"average_price_change\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "average_price_change.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Property Type Popularity:**\n",
    "\n",
    "Combine listing_df and calendar_df to find the most popular property type in Boston based on the number of bookings. Order the results by the number of bookings in descending order.\n",
    "from pyspark.sql.functions import count, desc\n",
    "\n",
    "# Assuming you have a \"property_type\" column in listing_df\n",
    "listing_df_q = listing_df.withColumnRenamed(\"property_type\", \"PropertyType\")\n",
    "\n",
    "\n",
    "# Join listing_df_q and boston_calendar_df on \"listing_id\"\n",
    "combined_df = calender_df.join(listing_df_q, calender_df.listing_id  == listing_df_q.id, \"inner\")\n",
    "\n",
    "# Group by \"PropertyType\" and count the number of bookings for each property type\n",
    "property_type_counts = combined_df.groupBy(\"PropertyType\") \\\n",
    "    .agg(count(\"*\").alias(\"BookingCount\"))\n",
    "\n",
    "# Order the results by the number of bookings in descending order\n",
    "property_type_counts = property_type_counts.orderBy(desc(\"BookingCount\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "property_type_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Question 3 - Superhosts and Response Times:**\n",
    "Using groupBy, count the number of superhosts in Boston by their response time categories. Rename the resulting columns to \"Response Time\" and \"Superhost Count.\"\n",
    "\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have columns named \"host_response_time,\" \"host_is_superhost,\" and \"city\"\n",
    "listing_df_q = listing_df.withColumnRenamed(\"host_response_time\", \"Response Time\")\n",
    "listing_df_q = listing_df_q.withColumnRenamed(\"host_is_superhost\", \"Superhost\")\n",
    "\n",
    "# Filter the DataFrame to select only superhosts in Boston\n",
    "boston_superhosts_df = listing_df_q.filter((col(\"Superhost\") == \"true\") & (col(\"city\") == \"Boston\"))\n",
    "\n",
    "# Group by \"Response Time\" and count the number of superhosts in each category\n",
    "superhost_count_df = boston_superhosts_df.groupBy(\"Response Time\") \\\n",
    "    .agg(count(\"Superhost\").alias(\"Superhost Count\"))\n",
    "\n",
    "# Order the results by \"Superhost Count\" in descending order\n",
    "ordered_superhost_count_df = superhost_count_df.orderBy(col(\"Superhost Count\").desc())\n",
    "\n",
    "# Show the resulting DataFrame with the renamed columns and ordered results\n",
    "ordered_superhost_count_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Top 5 Superhosts with the Most Listings:** \n",
    "\n",
    "Filter the DataFrame to select only superhosts, group them by host_name, count the number of listings each superhost has, and order the results by the count in descending order. Show only the top 5 superhosts.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "listing_df_q2 = listing_df.withColumnRenamed(\"host_name\", \"HostName\")\n",
    "listing_df_q2 = listing_df_q2.withColumnRenamed(\"host_is_superhost\", \"Superhost\")\n",
    "\n",
    "# Filter the DataFrame to select only superhosts\n",
    "superhosts_df = listing_df_q2.filter(col(\"Superhost\") == \"true\")\n",
    "\n",
    "# Group by \"Host Name\" and count the number of listings for each superhost\n",
    "superhost_counts = superhosts_df.groupBy(\"HostName\") \\\n",
    "    .agg(count(\"*\").alias(\"ListingCount\"))\n",
    "\n",
    "# Order the results by \"Listing Count\" in descending order\n",
    "superhost_counts = superhost_counts.orderBy(col(\"ListingCount\").desc())\n",
    "\n",
    "# Show only the top 5 superhosts\n",
    "top_5_superhosts = superhost_counts.limit(5)\n",
    "top_5_superhosts.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
